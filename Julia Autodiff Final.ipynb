{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0362a9",
   "metadata": {},
   "source": [
    "### What\n",
    "\n",
    "In this notebook, we build a simple prototype of source-to-source reverse-mode automatic differentiation. This is the same strategy used by `Zygote.jl`.\n",
    "\n",
    "We opt to implement **reverse mode** automatic differentiation because it efficiently computes the gradient of functions with a (very) large number of inputs and a single output. This kind of problem shows up in all kinds of optimization, most notably machine learning.\n",
    "\n",
    "We also choose to do **source-to-source** transformation. This is substantially more complex to implement than dynamic systems, which build up a computation graph and then differentiate through it on every program run. The main benefit is that our gradient computation can be compiled ahead of time and optimized down by LLVM, instead of (expensively) re-built and (poorly) optimized every time.\n",
    "\n",
    "This is also better than trace-based systems, which usually end up with a huge number of traced operations. Since there are so many operations, it's infeasible to analyze with optimization algorithms requiring `O(N^2)` time, and we're unable to optimize effectively anyway. (source: Zygote paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1eda0",
   "metadata": {},
   "source": [
    "### Version 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd85f1",
   "metadata": {},
   "source": [
    "We begin by building a program to automatically differentiate `simple_math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a376ec21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_math (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function simple_math(x, y)\n",
    "    x * y + sin(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae2230",
   "metadata": {},
   "source": [
    "Our automatic differentiator operates at the SSA-level: it deals with a simplified structure, called static single-assignment form, where variables can only be assigned once, and control flow is simplified into only simple branch/goto statements.\n",
    "\n",
    "Julia does this pass [internally](https://docs.julialang.org/en/v1/devdocs/ssair/), but doesn't expose the details, so we use the `IRTools.jl` library instead. This is the same one used by Zygote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f9dc8",
   "metadata": {},
   "source": [
    "Converting `simple_math` to SSA form using `IRTools.jl`, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b1f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using IRTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4122d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1: (%1, %2, %3)\n",
       "  %4 = %2 * %3\n",
       "  %5 = Main.sin(%2)\n",
       "  %6 = %4 + %5\n",
       "  return %6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRTools.@code_ir simple_math(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467f0a9",
   "metadata": {},
   "source": [
    "If you look closely, this is functionally identical to `simple_math` above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4193b",
   "metadata": {},
   "source": [
    "So how do we differentiate it? Our algorithm is as follows:\n",
    "* Find the return value (`%6` here), and set its adjoint to 1.\n",
    "* Iterate through the program backwards and compute the adjoints `gN` for each `%N`.\n",
    "\n",
    "For example, given `%6 = %4 + %5`, we want to convert it (using the sum rule for derivatives) into\n",
    "```\n",
    "g4 += g6\n",
    "g5 += g6\n",
    "```\n",
    "which possible involves also creating `g4` and `g5`.\n",
    "\n",
    "As we iterate through computations, we also keep a map from variables to the single static assignment which contains their most recently adjoint (`x_to_gx` in the code).\n",
    "\n",
    "A quick note: Zygote constructs a gradient function and returns that:\n",
    "\n",
    "```\n",
    "return a, function(a) \n",
    "    ... compute gradient ...\n",
    "    end\n",
    "```\n",
    "\n",
    "whereas we just compute the gradient inline while we compute the value and return them both.\n",
    "\n",
    "```\n",
    "return a, ga\n",
    "```\n",
    "\n",
    "There's no particular reason for this difference, other than that it seemed harmless and it's easiest to learn if you don't clone things _exactly_!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6665e2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_1 (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d25e902",
   "metadata": {},
   "source": [
    "Anyway, what does this transformation look like in practice?\n",
    "\n",
    "The output of this transformation on `simple_math` is\n",
    "```\n",
    "1: (%1, %2, %3)\n",
    "  %4 = %2 * %3\n",
    "  %5 = Main.sin(%2)\n",
    "  %6 = %4 + %5\n",
    "  %7 = 1\n",
    "  %8 = 0\n",
    "  %9 = 0\n",
    "  %10 = %8 + %7\n",
    "  %11 = %9 + %7\n",
    "  %12 = 0\n",
    "  %13 = cos(%2)\n",
    "  %14 = %11 * %13\n",
    "  %15 = %12 + %14\n",
    "  %16 = 0\n",
    "  %17 = 0\n",
    "  %18 = %10 * %3\n",
    "  %19 = %15 + %18\n",
    "  %20 = %10 * %2\n",
    "  %21 = %17 + %20\n",
    "  %22 = Core.tuple(%6, %19, %21)\n",
    "  return %22\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30209fda",
   "metadata": {},
   "source": [
    "Running that SSA code, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649a530c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.010007503399555, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_1(simple_math, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4cdc0",
   "metadata": {},
   "source": [
    "We check the same result with Zygote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e658d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af4b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.010007503399555, 3.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient(simple_math, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3a30c",
   "metadata": {},
   "source": [
    "Hooray! This is as the same as what Zygote reports for our function, so we've computed the gradient of `simple_math` correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a67025",
   "metadata": {},
   "source": [
    "### Improvement 2: Extensibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c93ee5",
   "metadata": {},
   "source": [
    "Here's what our addition rule currently looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4fb789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = \"\" #ignore me!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5548a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn == \"Main.:+\"\n",
    "    # where is the adjoint of the assignee?\n",
    "    g_assignee = x_to_gx[var(assignee)]\n",
    "\n",
    "    # either get the variable that we've accumulated adjoints into so far, \n",
    "    # or if it doesn't exist, initialize it to zero.\n",
    "    g_arg1_old = get(x_to_gx, ex.args[2], IRTools.push!(ir, :(0)))\n",
    "    g_arg2_old = get(x_to_gx, ex.args[3], IRTools.push!(ir, :(0)))\n",
    "\n",
    "    # finally, add the new bit (g_assignee) and accumulate it into the new variable.\n",
    "    g_arg1_new = IRTools.push!(ir, :($(g_arg1_old) + $(g_assignee)))\n",
    "    g_arg2_new = IRTools.push!(ir, :($(g_arg2_old) + $(g_assignee)))\n",
    "\n",
    "    x_to_gx[ex.args[2]] = g_arg1_new\n",
    "    x_to_gx[ex.args[3]] = g_arg2_new\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7f006",
   "metadata": {},
   "source": [
    "And we've repeated nearly the exact same thing for our multiplication rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8e5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn == \"Main.:*\"\n",
    "    g_assignee = x_to_gx[var(assignee)]\n",
    "\n",
    "    g_arg1_old = get(x_to_gx, ex.args[2], IRTools.push!(ir, :(0)))\n",
    "    g_arg2_old = get(x_to_gx, ex.args[3], IRTools.push!(ir, :(0)))\n",
    "\n",
    "    g_arg1_new = IRTools.push!(ir, :($(g_arg1_old) + $(g_assignee) * $(ex.args[3])))\n",
    "    g_arg2_new = IRTools.push!(ir, :($(g_arg2_old) + $(g_assignee) * $(ex.args[2])))\n",
    "\n",
    "    x_to_gx[ex.args[2]] = g_arg1_new\n",
    "    x_to_gx[ex.args[3]] = g_arg2_new\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00978a34",
   "metadata": {},
   "source": [
    "Notice the similarity in the `g_arg1_new` line. They're almost exactly the same for `+` and `*`, with the only exception being the factor that `g_assignee` is multiplied by.\n",
    "\n",
    "We take advantage of this (general, it turns out) property to express all these rules more succintly with the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf657a4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: ex not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ex not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[11]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "if ex.head == :call\n",
    "    g_assignee = x_to_gx[var(assignee)]\n",
    "    for (i, a) in Iterators.enumerate(ex.args[2:length(ex.args)])\n",
    "        g_old = get(x_to_gx, a, IRTools.push!(ir, :(0)))\n",
    "        factor = derivative_rule(eval(ex.args[1]), ex.args[2: length(ex.args)], i)\n",
    "        g_new = IRTools.push!(ir, :($(g_old) + $(g_assignee) * $(factor)))\n",
    "        x_to_gx[a] = g_new\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26545ac",
   "metadata": {},
   "source": [
    "The magic happens in `derivative_rule` via multiple dispatch. For each operation, we implement the derivative rule with respect to the `i`th argument. Here are the three functions we implemented in version 1. Notice how much more succinct and self-contained they are this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd7cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_rule (generic function with 3 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function derivative_rule(::typeof(sin), args, i)\n",
    "    return :(cos($(args[1])))\n",
    "end\n",
    "\n",
    "function derivative_rule(::typeof(+), args, i)\n",
    "    return 1\n",
    "end\n",
    "\n",
    "# not quite the * rule in full generality, but good enough for our two-argument purposes.\n",
    "function derivative_rule(::typeof(*), args, i)\n",
    "    if i == 1\n",
    "        return args[2]\n",
    "    elseif i == 2\n",
    "        return args[1]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10348485",
   "metadata": {},
   "source": [
    "And just to make sure our differentiator still works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a690fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.010007503399555, 3)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff_v2.jl\")\n",
    "gradient_2(simple_math, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96918afc",
   "metadata": {},
   "source": [
    "Great! Now we have a basic reverse mode automatic differentiator, capable of handling long sequences of manipulations of primitive operations. And if the user wants to use a new function, then can define their own derivative_rule for it without reaching into the internals of our library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abae6d8",
   "metadata": {},
   "source": [
    "### Improvement 3: Recursive decent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096e8ea",
   "metadata": {},
   "source": [
    "Consider the following series of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3747f643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mult (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mult(a, b)\n",
    "    a * b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68f548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add(a, b)\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d2f8560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "composite (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function composite(a, b)\n",
    "    mult(a, add(a, b))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faf86ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function derivative_rule(::typeof(mult), args, i)\n",
    "#    return gradient_2(mult, args...)[i]\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bdb0b",
   "metadata": {},
   "source": [
    "Our current implementation will fail to differentiate `composite` (with `MethodError: no method matching derivative_rule(::typeof(mult), ::Vector{Any}, ::Int64)\n",
    "`) since `add` and `mult` aren't functions it knows how to differentiate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719ad680",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] ##318",
      "    @ ./In[20]:2 [inlined]",
      "  [2] var\"##318\"(arg1::Nothing, arg2::IRTools.Inner.Variable, arg3::IRTools.Inner.Variable)",
      "    @ IRTools.Inner ~/.julia/packages/IRTools/017wp/src/eval.jl:0",
      "  [3] invokelatest(::Any, ::Any, ::Vararg{Any}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Base ./essentials.jl:729",
      "  [4] invokelatest(::Any, ::Any, ::Vararg{Any})",
      "    @ Base ./essentials.jl:726",
      "  [5] gradient_2(::Function, ::IRTools.Inner.Variable, ::Vararg{IRTools.Inner.Variable})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v2.jl:51",
      "  [6] derivative_rule(unknown_function::Function, args::Vector{Any}, i::Int64)",
      "    @ Main ./In[18]:2",
      "  [7] gradient_2(::Function, ::Int64, ::Vararg{Int64})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v2.jl:29",
      "  [8] top-level scope",
      "    @ In[24]:1",
      "  [9] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "gradient_2(composite, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d0a791",
   "metadata": {},
   "source": [
    "But if we can teach it to recursively descend into `add` and `mult`, it'll discover that they're made of functions we recognize, so we should be able to differentiate the entire thing.\n",
    "\n",
    "That seems simple: we'll just introduce a simple fallback function, like this, which catches derivatives of previously unseen functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f397479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_rule (generic function with 4 methods)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function derivative_rule(unknown_function, args, i)\n",
    "    return gradient_2(unknown_function, args...)[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2b1a62",
   "metadata": {},
   "source": [
    "Then, instead of computing `derivative_rule` at compile time, we'll insert the `derivative_rule` symbolically and compute it at runtime. This should allow us to do things like infinite recursion without issue. Here's some SSA code generated by `gradient_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebaeff22",
   "metadata": {},
   "source": [
    "```\n",
    "1: (%1, %2, %3)\n",
    "  %4 = %2 * %3\n",
    "  %5 = 1\n",
    "  %6 = 0\n",
    "  %7 = derivative_rule(*, Any[%2, %3], i)\n",
    "  %8 = %5 * %7\n",
    "  %9 = %6 + %8\n",
    "  %10 = 0\n",
    "  %11 = derivative_rule(*, Any[%2, %3], i)\n",
    "  %12 = %5 * %11\n",
    "  %13 = %10 + %12\n",
    "  %14 = Core.tuple(%4, %9, %13)\n",
    "  return %14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "683b5cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_rule (generic function with 4 methods)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff_v3.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9df15d",
   "metadata": {},
   "source": [
    "Now we go to call our new SSA code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ef36ab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] ##320",
      "    @ ./In[20]:2 [inlined]",
      "  [2] var\"##320\"(arg1::Nothing, arg2::IRTools.Inner.Variable, arg3::IRTools.Inner.Variable)",
      "    @ IRTools.Inner ~/.julia/packages/IRTools/017wp/src/eval.jl:0",
      "  [3] invokelatest(::Any, ::Any, ::Vararg{Any}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Base ./essentials.jl:729",
      "  [4] invokelatest(::Any, ::Any, ::Vararg{Any})",
      "    @ Base ./essentials.jl:726",
      "  [5] gradient_3(::Function, ::IRTools.Inner.Variable, ::Vararg{IRTools.Inner.Variable})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v3.jl:59",
      "  [6] derivative_rule(unknown_function::Function, args::Vector{Any}, i::Int64)",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v3.jl:89",
      "  [7] gradient_3(::Function, ::Int64, ::Vararg{Int64})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v3.jl:31",
      "  [8] top-level scope",
      "    @ In[27]:1",
      "  [9] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "gradient_3(composite, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddee264",
   "metadata": {},
   "source": [
    "Agh! We got `MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n",
    "`. So we successfully recursed into `mult` and `add` and transformed the syntax, but then something fell apart when we tried to run it. What happened?\n",
    "\n",
    "Here's where we run into one of the first real issues with our divergence from Zygote's design.\n",
    "\n",
    "Our design choice was to build around a function `gradient`, which takes a function (`composite`) and two values (`3` and `5`), and returns the gradients of each argument. But we encounter a problem: internally, we need to be able to compute the gradient without inputting actual values. That means that we need to insert the `derivative_rule` function as a symbol, which is fine. But IRTools doesn't seem to support \"deep-resolving\" variables in function calls like we'd require to succesfully pass in and then resolve e.g. lists of symbolic variables.\n",
    "\n",
    "For example, I was hoping for / expecting `%7 = derivative_rule(*, Any[%2, %3], i)` to pass in the values for `%2` and `%3` at runtime, but instead we pass in the literal `::IRTools.Inner.Variable`. And then that throws `MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)`, as expected.\n",
    "\n",
    "My current understanding is that this is a somewhat unfortunate implementation shortcoming but not fundamental limitation of `IRTools`. However, it wouldn't surprise me if there's some more fundamental reason this is impossible that I'm not seeing here.\n",
    "\n",
    "Zygote resolves this problem by building around a function called a pullback `J`, which returns an output and a gradient function. Then it can symbolically call those gradient functions in its newly constructed gradient function without issue.\n",
    "\n",
    "This is a bummer, because it means we're unlikely to be able to pull off recursion, which fundamentally requires the same trick (defer `derivative_rule` evaluation until runtime)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb125df",
   "metadata": {},
   "source": [
    "### Improvement 4: Conditional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b89bfb9",
   "metadata": {},
   "source": [
    "Ok, we got unexpectedly bitten by our cute design choice in recursive descent, but can we pull off conditionals?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d81852",
   "metadata": {},
   "source": [
    "Let's look at a program that includes a conditional return, like this one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6c46368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conditional (generic function with 1 method)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function conditional(a, b)\n",
    "    c = a * b\n",
    "    if a >= b\n",
    "        return a * a\n",
    "    else\n",
    "        return c * sin(c)\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e72d7e",
   "metadata": {},
   "source": [
    "Here's its SSA-form IR, which now has basic control flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbda8411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1: (%1, %2, %3)\n",
       "  %4 = %2 * %3\n",
       "  %5 = %2 >= %3\n",
       "  br 3 unless %5\n",
       "2:\n",
       "  %6 = %2 * %2\n",
       "  return %6\n",
       "3:\n",
       "  %7 = Main.sin(%4)\n",
       "  %8 = %4 * %7\n",
       "  return %8"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRTools.@code_ir conditional(1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e3594d",
   "metadata": {},
   "source": [
    "Recall that our earlier algorithm starts at the return value, and then propagates the adjoints up from there. That's immediately a problem for us, since by symbolically analyzing this program, we can't tell whether `return %6` or `return %8` will ultimately be run!\n",
    "\n",
    "To deal with `N` conditional returns, we could just compute N separate gradients, and then return the one we need. This would be wasteful, since reverse mode autodiff needs to do a pass for every output. \n",
    "\n",
    "Instead, we trace control flow (and only control flow! No expensive per-operator traces allowed) take notice of which branches actually run, and then only propagate adjoints through those variables. This is also Zygote's solution.\n",
    "\n",
    "Since Zygote constructs the gradient function in the function itself, they can collect an inline record of what control flow ran. Since our implementation is slightly different, we first generate an instrumented function, run it to extract the control flow, and then use that to determine which adjoints should contribute to the final output.\n",
    "\n",
    "\n",
    "But oh no! We get bitten hard by our design decision again. We need to know the final path of control flow to compute our gradients, but we compute our gradients inline. That means that we need to run the function twice (once to extract control flow, and again to calculate gradients), and we incur any side effects twice. If I had realized this at the beginning, I would have just implemented `J` Zygote's way; it's not particularly easier or harder, and it's now clear to me that it has very substantial benefits!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d733328d",
   "metadata": {},
   "source": [
    "Enough talk. Let's see some tracing!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21306f5",
   "metadata": {},
   "source": [
    "We handle the tracing by writing `trace_executed_blocks`, which annotates the IR like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a84974c",
   "metadata": {},
   "source": [
    "```\n",
    "1: (%1, %2, %3)\n",
    "  %10 = push!(Main.gradient_trace, 1)\n",
    "  %4 = %2 * %3\n",
    "  %5 = %2 >= %3\n",
    "  br 3 unless %5\n",
    "2:\n",
    "  %11 = push!(Main.gradient_trace, 2)\n",
    "  %6 = %2 * %2\n",
    "  return %6\n",
    "3:\n",
    "  %12 = push!(Main.gradient_trace, 3)\n",
    "  %7 = %2 * %3\n",
    "  %8 = Main.sin(%7)\n",
    "  %9 = %7 * %8\n",
    "  return %9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965459a",
   "metadata": {},
   "source": [
    "`trace_executed_blocks` then returns the blocks that executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2faa91b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trace_executed_blocks (generic function with 1 method)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff_v4.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8c10242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Any}:\n",
       " 1\n",
       " 3"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_executed_blocks(conditional, 3, 5) # blocks 1 and 3 run when a >= b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d15f3c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Any}:\n",
       " 1\n",
       " 2"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_executed_blocks(conditional, 5, 3) # blocks 1 and 2 run when a < b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc04dd4",
   "metadata": {},
   "source": [
    "Now that we know which blocks ran, we need to parse the IR to determine which statements to propagate the adjoint through. We find the block enclosed by each statement (and each return value), and only compute gradients for those variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dced3b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dict{Any, Any}(5 => 1, 4 => 1, 6 => 2, 7 => 3, 9 => 3, 8 => 3), Dict{Any, Any}(2 => 6, 3 => 9))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statements_to_surrounding_block(conditional, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0ed92e",
   "metadata": {},
   "source": [
    "Then we also need to mangle the block/goto structure, since I don't think IRTools supports inserting SSA code in arbitrary blocks (or if it does, I couldn't figure out how to do it!). So we re-arrange the block structure to put the \"true\" return statement last, and then inject all our suffix code (dealing with returning the gradient) right at the end of the program.\n",
    "\n",
    "Side note: IRTools doesn't seem to support a native way to do these branch/block analyses, so we end up using regexes (!) on the stringification. Yuck! :p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03f9b4c",
   "metadata": {},
   "source": [
    "Here's what the traced function looks like:\n",
    "```\n",
    "\n",
    "Dict{Any, Any}(5 => 1, 4 => 1, 6 => 2, 7 => 3, 9 => 3, 8 => 3) # statement => block\n",
    "Any[1, 2] # blocks that were run\n",
    "Dict{Any, Any}(2 => 6, 3 => 9) # block => return variable\n",
    "\n",
    "skipped 9 # these variabels don't contribute, so we skip them\n",
    "skipped 8\n",
    "skipped 7\n",
    "1: (%1, %2, %3)\n",
    "  %4 = %2 * %3\n",
    "  %5 = %2 >= %3\n",
    "  br 1 unless %5 # notice that this has been mangled to avoid a dangling reference to block 3\n",
    "2:\n",
    "  %6 = %2 * %2\n",
    "  %10 = 1\n",
    "  %11 = 0\n",
    "  %12 = %10 * %2\n",
    "  %13 = %11 + %12\n",
    "  %14 = 0\n",
    "  %15 = %10 * %2\n",
    "  %16 = %13 + %15\n",
    "  %17 = nothing\n",
    "  %18 = Core.tuple(%6, %16, %17)\n",
    "  return %18\n",
    "  \n",
    "  # notice that block 3 is now gone\n",
    "```\n",
    "\n",
    "That's all great, but does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80445ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict{Any, Any}(5 => 1, 4 => 1, 6 => 2, 7 => 3, 8 => 3)Any[1, 2]Dict{Any, Any}(2 => 6, 3 => 8)\n",
      "skipped 8\n",
      "skipped 7\n",
      "1: (%1, %2, %3)\n",
      "  %4 = %2 * %3\n",
      "  %5 = %2 >= %3\n",
      "  br 1 unless %5\n",
      "2:\n",
      "  %6 = %2 * %2\n",
      "  %9 = 1\n",
      "  %10 = 0\n",
      "  %11 = %9 * %2\n",
      "  %12 = %10 + %11\n",
      "  %13 = 0\n",
      "  %14 = %9 * %2\n",
      "  %15 = %12 + %14\n",
      "  %16 = nothing\n",
      "  %17 = Core.tuple(%6, %15, %16)\n",
      "  return %17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10, nothing)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_4(conditional, 5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "edd926b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, nothing)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient(conditional, 5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad0b532",
   "metadata": {},
   "source": [
    "Amazing! It does! But that conditional was kind of easy; it only had two cases, and its conditions and computations were simple. What if we push a bit harder?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "384d6552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conditional_hard (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function conditional_hard(a, b)\n",
    "    c = a * b * sin(b) * sin(a)\n",
    "    \n",
    "    if a >= b * 3\n",
    "        return a * a\n",
    "        \n",
    "    elseif a >= b\n",
    "        c = a * b\n",
    "        return c * sin(c)\n",
    "        \n",
    "    elseif a > sin(b)\n",
    "        return sin(b)\n",
    "        \n",
    "    else\n",
    "        e = sin(sin(sin(a)) * b) * a\n",
    "        \n",
    "    end    \n",
    "end     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3b8e91a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10.0, nothing), (-32.23509255817561, -53.725154263626024), (nothing, 1.0), (7.638088029360477, 0.38494624699166546))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient(conditional_hard, 5, 1), \n",
    "Zygote.gradient(conditional_hard, 5, 3), \n",
    "Zygote.gradient(conditional_hard, 5, 10π), \n",
    "Zygote.gradient(conditional_hard, -3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "059b0984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dict{Any, Any}(5 => 1, 16 => 6, 20 => 7, 12 => 4, 8 => 1, 17 => 7, 19 => 7, 6 => 1, 11 => 4, 9 => 2, 14 => 5, 7 => 1, 4 => 1, 13 => 4, 15 => 5, 21 => 7, 10 => 3, 18 => 7)Any[1, 2]Dict{Any, Any}(4 => 13, 6 => 16, 7 => 21, 2 => 9)\n",
      "skipped 21\n",
      "skipped 20\n",
      "skipped 19\n",
      "skipped 18\n",
      "skipped 17\n",
      "skipped 16\n",
      "skipped 15\n",
      "skipped 14\n",
      "skipped 13\n",
      "skipped 12\n",
      "skipped 11\n",
      "skipped 10\n",
      "1: (%1, %2, %3)\n",
      "  %4 = Main.sin(%3)\n",
      "  %5 = Main.sin(%2)\n",
      "  %6 = %2 * %3 * %4 * %5\n",
      "  %7 = %3 * 3\n",
      "  %8 = %2 >= %7\n",
      "  br 1 unless %8\n",
      "2:\n",
      "  %9 = %2 * %2\n",
      "  %22 = 1\n",
      "  %23 = 0\n",
      "  %24 = %22 * %2\n",
      "  %25 = %23 + %24\n",
      "  %26 = 0\n",
      "  %27 = %22 * %2\n",
      "  %28 = %25 + %27\n",
      "  %29 = nothing\n",
      "  %30 = Core.tuple(%9, %28, %29)\n",
      "  return %30\n",
      "Dict{Any, Any}(5 => 1, 16 => 6, 20 => 7, 12 => 4, 8 => 1, 17 => 7, 19 => 7, 6 => 1, 11 => 4, 9 => 2, 14 => 5, 7 => 1, 4 => 1, 13 => 4, 15 => 5, 21 => 7, 10 => 3, 18 => 7)Any[1, 3, 4]Dict{Any, Any}(4 => 13, 6 => 16, 7 => 21, 2 => 9)\n",
      "skipped 21\n",
      "skipped 20\n",
      "skipped 19\n",
      "skipped 18\n",
      "skipped 17\n",
      "skipped 16\n",
      "skipped 15\n",
      "skipped 14\n",
      "skipped 9\n",
      "1: (%1, %2, %3)\n",
      "  %4 = Main.sin(%3)\n",
      "  %5 = Main.sin(%2)\n",
      "  %6 = %2 * %3 * %4 * %5\n",
      "  %7 = %3 * 3\n",
      "  %8 = %2 >= %7\n",
      "  br 3 unless %8\n",
      "2:\n",
      "  %9 = %2 * %2\n",
      "  br 1 (%9)\n",
      "3:\n",
      "  %10 = %2 >= %3\n",
      "  br 1 unless %10\n",
      "4:\n",
      "  %11 = %2 * %3\n",
      "  %12 = Main.sin(%11)\n",
      "  %13 = %11 * %12\n",
      "  %22 = 1\n",
      "  %23 = 0\n",
      "  %24 = %22 * %12\n",
      "  %25 = %23 + %24\n",
      "  %26 = 0\n",
      "  %27 = %22 * %11\n",
      "  %28 = %26 + %27\n",
      "  %29 = 0\n",
      "  %30 = cos(%11)\n",
      "  %31 = %28 * %30\n",
      "  %32 = %25 + %31\n",
      "  %33 = 0\n",
      "  %34 = %32 * %3\n",
      "  %35 = %33 + %34\n",
      "  %36 = 0\n",
      "  %37 = %32 * %2\n",
      "  %38 = %36 + %37\n",
      "  %39 = nothing\n",
      "  %40 = Core.tuple(%13, %35, %38)\n",
      "  return %40\n",
      "Dict{Any, Any}(5 => 1, 16 => 6, 20 => 7, 12 => 4, 8 => 1, 17 => 7, 19 => 7, 6 => 1, 11 => 4, 9 => 2, 14 => 5, 7 => 1, 4 => 1, 13 => 4, 15 => 5, 21 => 7, 10 => 3, 18 => 7)Any[1, 3, 5, 6]Dict{Any, Any}(4 => 13, 6 => 16, 7 => 21, 2 => 9)\n",
      "skipped 21\n",
      "skipped 20\n",
      "skipped 19\n",
      "skipped 18\n",
      "skipped 17\n",
      "skipped 13\n",
      "skipped 12\n",
      "skipped 11\n",
      "skipped 9\n",
      "1: (%1, %2, %3)\n",
      "  %4 = Main.sin(%3)\n",
      "  %5 = Main.sin(%2)\n",
      "  %6 = %2 * %3 * %4 * %5\n",
      "  %7 = %3 * 3\n",
      "  %8 = %2 >= %7\n",
      "  br 3 unless %8\n",
      "2:\n",
      "  %9 = %2 * %2\n",
      "  br 1 (%9)\n",
      "3:\n",
      "  %10 = %2 >= %3\n",
      "  br 5 unless %10\n",
      "4:\n",
      "  %11 = %2 * %3\n",
      "  %12 = Main.sin(%11)\n",
      "  %13 = %11 * %12\n",
      "  br 1 (%13)\n",
      "5:\n",
      "  %14 = Main.sin(%3)\n",
      "  %15 = %2 > %14\n",
      "  br 1 unless %15\n",
      "6:\n",
      "  %16 = Main.sin(%3)\n",
      "  %22 = 1\n",
      "  %23 = 0\n",
      "  %24 = cos(%3)\n",
      "  %25 = %22 * %24\n",
      "  %26 = %23 + %25\n",
      "  %27 = nothing\n",
      "  %28 = Core.tuple(%16, %27, %26)\n",
      "  return %28\n",
      "Dict{Any, Any}(5 => 1, 16 => 6, 20 => 7, 12 => 4, 8 => 1, 17 => 7, 19 => 7, 6 => 1, 11 => 4, 9 => 2, 14 => 5, 7 => 1, 4 => 1, 13 => 4, 15 => 5, 21 => 7, 10 => 3, 18 => 7)Any[1, 3, 5, 7]Dict{Any, Any}(4 => 13, 6 => 16, 7 => 21, 2 => 9)\n",
      "skipped 16\n",
      "skipped 13\n",
      "skipped 12\n",
      "skipped 11\n",
      "skipped 9\n",
      "1: (%1, %2, %3)\n",
      "  %4 = Main.sin(%3)\n",
      "  %5 = Main.sin(%2)\n",
      "  %6 = %2 * %3 * %4 * %5\n",
      "  %7 = %3 * 3\n",
      "  %8 = %2 >= %7\n",
      "  br 3 unless %8\n",
      "2:\n",
      "  %9 = %2 * %2\n",
      "  br 1 (%9)\n",
      "3:\n",
      "  %10 = %2 >= %3\n",
      "  br 5 unless %10\n",
      "4:\n",
      "  %11 = %2 * %3\n",
      "  %12 = Main.sin(%11)\n",
      "  %13 = %11 * %12\n",
      "  br 1 (%13)\n",
      "5:\n",
      "  %14 = Main.sin(%3)\n",
      "  %15 = %2 > %14\n",
      "  br 7 unless %15\n",
      "6:\n",
      "  %16 = Main.sin(%3)\n",
      "  br 1 (%16)\n",
      "7:\n",
      "  %17 = Main.sin(%2)\n",
      "  %18 = Main.sin(%17)\n",
      "  %19 = %18 * %3\n",
      "  %20 = Main.sin(%19)\n",
      "  %21 = %20 * %2\n",
      "  %22 = 1\n",
      "  %23 = 0\n",
      "  %24 = %22 * %2\n",
      "  %25 = %23 + %24\n",
      "  %26 = 0\n",
      "  %27 = %22 * %20\n",
      "  %28 = %26 + %27\n",
      "  %29 = 0\n",
      "  %30 = cos(%19)\n",
      "  %31 = %25 * %30\n",
      "  %32 = %29 + %31\n",
      "  %33 = 0\n",
      "  %34 = %32 * %3\n",
      "  %35 = %33 + %34\n",
      "  %36 = 0\n",
      "  %37 = %32 * %18\n",
      "  %38 = %36 + %37\n",
      "  %39 = 0\n",
      "  %40 = cos(%17)\n",
      "  %41 = %35 * %40\n",
      "  %42 = %39 + %41\n",
      "  %43 = 0\n",
      "  %44 = cos(%2)\n",
      "  %45 = %42 * %44\n",
      "  %46 = %28 + %45\n",
      "  %47 = nothing\n",
      "  %48 = Core.tuple(%21, %46, %38)\n",
      "  return %48\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((10, nothing), (-32.23509255817561, -53.725154263626024), (nothing, 1.0), (7.638088029360477, 0.38494624699166546))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_4(conditional_hard, 5, 1), \n",
    "gradient_4(conditional_hard, 5, 3), \n",
    "gradient_4(conditional_hard, 5, 10π), \n",
    "gradient_4(conditional_hard, -3, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301c249",
   "metadata": {},
   "source": [
    "The output, `((10, nothing), (-32.23509255817561, -53.725154263626024), (nothing, 1.0), (7.638088029360477, 0.38494624699166546))`, is exactly the same in all four cases! Hooray! (This also confirms that our original reverse AD code above is reasonably robust.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99209e",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project started out with the intention of writing an AD system like `Zygote.jl`. It's no fun to clone things *exactly*, so I figured that instead of returning a value and a gradient (pullback) function, as Zygote does, I'd just compute both the value and the gradient values inline and then return both. To do so, I'd manipulate a simplified syntax (SSA-form generated via `IRTools`), as Zygote does, but inject my gradient calculations between each line instead of all at the end.\n",
    "\n",
    "This works well up to computing the derivative of functions composed of functions whose derivatives we know. But it falls apart when we try to compose functions with unknown derivatives, due to what I suspect is a limitation of `IRTools`.\n",
    "\n",
    "Nevertheless, we continue to successfully implement extensibility and support for conditional returns.\n",
    "\n",
    "When I started this project I was expecting to learn a lot about automatic differentiators. And in some sense I did--I now have a pretty good understanding of the design space tradeoffs and where Zygote (and several other popular AD systems) sit in it. \n",
    "\n",
    "But really I spent most of my time (a way larger % than expected) learning how to do metaprogramming and debugging a small number of extremely dense snippets (for example, the six lines of code inside `if ex.head == :call` took a veritable eternity to get correct.) Lots of quoting and unquoting issues in all directions.\n",
    "\n",
    "I'm pretty happy with this, since metaprogramming is increasingly something that seems important for me to master, but probably a bit less actual AD meat involved than I originally expected. So it goes!\n",
    "\n",
    "If you want to see my scratch work for any of this, there are several other Jupyter notebooks included in this repo for your browswing :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b7af33",
   "metadata": {},
   "source": [
    "### Quick note on effort allocation\n",
    "I figured I'd write a quick note on where exactly my ~3 psets worth of time and effort went on this project, since some parts took longer than expected and some shorter than expected.\n",
    "\n",
    "* Understanding how to implement reverse mode AD was pretty fast. I read up on it and did a few chalkboard examples, and then felt like I understood it. ~2 hours.\n",
    "* Really understanding Julia's metaprogramming (quoting, `$()`, `eval`, etc) took ~2 hours. It's been a while since I've dealt with a lispy language.\n",
    "* I spent 2-3 hours trying to inspect Zygote itself with a debugger to understand it, but this was a nightmare. One of the questions I'd like to ask someone in the Julia lab is how the pros inspect libraries in Julia. Generated functions, multiple dispatch, etc., made it extremely hard for me to track down who called whom. After lots of trying I gave up on this approach.\n",
    "* I spent 2 hours reading and trying to understand the Zygote paper, which was actually pretty useful. In particular, when I started this project, I didn't really understand the design tradeoffs made by Zygote vs e.g. PyTorch.\n",
    "* I spent ~8 hours learning `IRTools` inside out, and debugging various issues with it. For example, it doesn't correctly parse injected expressions like `Core.tuple(outs)`, since it thinks the dot is a function call, so instead we're using `IRTools.Inner.Statement(:($(GlobalRef(Core, :tuple))($(outs...),)))`, which took me forever to figure out. This was a much, much larger timesink than I expected or is probably obvious from looking at the project.\n",
    "* There's probably an hour or two of learning how to use Julia at all (this is my first Julia project!).\n",
    "* The real AD coding (most of what's in this notebook) probably took about 15 hours, exluding IRTools and Julia language time.\n",
    "* I spent 1-2 total hours writing and another hour arranging code to make the history of the project clear to you, dear grader! :)\n",
    "\n",
    "This seems roughly equivalent to three heavier psets of work, but I'm very happy with what I've practiced here and generally how I've spent this time, if you're happy with the output. Thanks for letting me branch out and do my own project!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbfc5f",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03859ee7",
   "metadata": {},
   "source": [
    "* https://arxiv.org/pdf/1810.07951.pdf. This is the original Zygote paper, and provides useful context on the tradeoffs Zygote makes in design space.\n",
    "* https://fluxml.ai/Zygote.jl/latest/internals/. This page gives an overview of how Zygote works, and includes an example of a loop being unrolled.\n",
    "* https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation. This post helped me deeply understand how reverse-mode autodiff works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290c277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
