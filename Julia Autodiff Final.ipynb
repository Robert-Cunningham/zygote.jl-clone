{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f0362a9",
   "metadata": {},
   "source": [
    "### What\n",
    "\n",
    "In this notebook, we build a simple prototype of source-to-source reverse-mode automatic differentiation. This is the same strategy used by `Zygote.jl`.\n",
    "\n",
    "We opt to implement **reverse mode** automatic differentiation because it efficiently computes the gradient of functions with a (very) large number of inputs and a single output. This kind of problem shows up in all kinds of optimization, most notably machine learning.\n",
    "\n",
    "We also choose to do **source-to-source** transformation. This is substantially more complex to implement than dynamic systems, which build up a computation graph and then differentiate through it on every program run. The main benefit is that our gradient computation can be compiled ahead of time and optimized down by LLVM, instead of (expensively) re-built and (poorly) optimized every time.\n",
    "\n",
    "This is also better than trace-based systems, which usually end up with a huge number of traced operations. Since there are so many operations, it's infeasible to analyze with optimization algorithms requiring `O(N^2)` time, and we're unable to optimize effectively anyway. (source: Zygote paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa1eda0",
   "metadata": {},
   "source": [
    "### Version 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd85f1",
   "metadata": {},
   "source": [
    "We begin by building a program to automatically differentiate `simple_math`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a376ec21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simple_math (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function simple_math(x, y)\n",
    "    x * y + sin(x)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae2230",
   "metadata": {},
   "source": [
    "Our automatic differentiator operates at the SSA-level: it deals with a simplified structure, called static single-assignment form, where variables can only be assigned once, and control flow is simplified into only simple branch/goto statements.\n",
    "\n",
    "Julia does this pass [internally](https://docs.julialang.org/en/v1/devdocs/ssair/), but doesn't expose the details, so we use the `IRTools.jl` library instead. This is the same one used by Zygote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676f9dc8",
   "metadata": {},
   "source": [
    "Converting `simple_math` to SSA form using `IRTools.jl`, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28b1f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "using IRTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4122d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1: (%1, %2, %3)\n",
       "  %4 = %2 * %3\n",
       "  %5 = Main.sin(%2)\n",
       "  %6 = %4 + %5\n",
       "  return %6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IRTools.@code_ir simple_math(3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d4193b",
   "metadata": {},
   "source": [
    "For our first version, our algorithm is as follows:\n",
    "* Find the return value (`%6` here), and set its adjoint to 1.\n",
    "* Iterate through the program backwards and compute the adjoints `gN` for each `%N`.\n",
    "\n",
    "For example, given `%6 = %4 + %5`, we want to convert it (using the sum rule for derivatives) into\n",
    "```\n",
    "g4 += g6\n",
    "g5 += g6\n",
    "```\n",
    "which possible involves also creating `g4` and `g5`.\n",
    "\n",
    "As we iterate through computations, we also keep a map from variables to the single static assignment which contains their most recently adjoint (`x_to_gx` in the code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6665e2ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gradient_1 (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd808925",
   "metadata": {},
   "source": [
    "If you want to see it explicitly, the output of this transformation on `simple_math` is\n",
    "```\n",
    "1: (%1, %2, %3)\n",
    "  %4 = %2 * %3\n",
    "  %5 = Main.sin(%2)\n",
    "  %6 = %4 + %5\n",
    "  %7 = 1\n",
    "  %8 = 0\n",
    "  %9 = 0\n",
    "  %10 = %8 + %7\n",
    "  %11 = %9 + %7\n",
    "  %12 = 0\n",
    "  %13 = cos(%2)\n",
    "  %14 = %11 * %13\n",
    "  %15 = %12 + %14\n",
    "  %16 = 0\n",
    "  %17 = 0\n",
    "  %18 = %10 * %3\n",
    "  %19 = %15 + %18\n",
    "  %20 = %10 * %2\n",
    "  %21 = %17 + %20\n",
    "  %22 = Core.tuple(%6, %19, %21)\n",
    "  return %22\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "649a530c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.010007503399555, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient_1(simple_math, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb4cdc0",
   "metadata": {},
   "source": [
    "We check the same result with Zygote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e658d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af4b2fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.010007503399555, 3.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Zygote.gradient(simple_math, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3a30c",
   "metadata": {},
   "source": [
    "Hooray! This is as the same as what Zygote reports for our function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a67025",
   "metadata": {},
   "source": [
    "### Improvement 2: Generality & Extensibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c93ee5",
   "metadata": {},
   "source": [
    "Here's what our addition rule currently looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da4fb789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5548a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn == \"Main.:+\"\n",
    "    # where is the adjoint of the assignee?\n",
    "    g_assignee = x_to_gx[var(assignee)]\n",
    "\n",
    "    # either get the variable that we've accumulated adjoints into so far, \n",
    "    # or if it doesn't exist, initialize it to zero.\n",
    "    g_arg1_old = get(x_to_gx, ex.args[2], IRTools.push!(ir, :(0)))\n",
    "    g_arg2_old = get(x_to_gx, ex.args[3], IRTools.push!(ir, :(0)))\n",
    "\n",
    "    # finally, add the new bit (g_assignee) and accumulate it into the new variable.\n",
    "    g_arg1_new = IRTools.push!(ir, :($(g_arg1_old) + $(g_assignee)))\n",
    "    g_arg2_new = IRTools.push!(ir, :($(g_arg2_old) + $(g_assignee)))\n",
    "\n",
    "    x_to_gx[ex.args[2]] = g_arg1_new\n",
    "    x_to_gx[ex.args[3]] = g_arg2_new\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b7f006",
   "metadata": {},
   "source": [
    "And we've repeated nearly the exact same thing for our multiplication rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8e5aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fn == \"Main.:*\"\n",
    "    g_assignee = x_to_gx[var(assignee)]\n",
    "\n",
    "    g_arg1_old = get(x_to_gx, ex.args[2], IRTools.push!(ir, :(0)))\n",
    "    g_arg2_old = get(x_to_gx, ex.args[3], IRTools.push!(ir, :(0)))\n",
    "\n",
    "    g_arg1_new = IRTools.push!(ir, :($(g_arg1_old) + $(g_assignee) * $(ex.args[3])))\n",
    "    g_arg2_new = IRTools.push!(ir, :($(g_arg2_old) + $(g_assignee) * $(ex.args[2])))\n",
    "\n",
    "    x_to_gx[ex.args[2]] = g_arg1_new\n",
    "    x_to_gx[ex.args[3]] = g_arg2_new\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00978a34",
   "metadata": {},
   "source": [
    "Notice the similarity in the `g_arg1_new` line. They're almost exactly the same for `+` and `*`, with the only exception being the factor that `g_assignee` is multiplied by.\n",
    "\n",
    "We take advantage of this (general, it turns out) property to express all these rules more succintly with the following snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf657a4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: ex not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ex not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ ./In[36]:1",
      " [2] eval",
      "   @ ./boot.jl:368 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "if ex.head == :call\n",
    "    g_assignee = x_to_gx[var(assignee)]\n",
    "    for (i, a) in Iterators.enumerate(ex.args[2:length(ex.args)])\n",
    "        g_old = get(x_to_gx, a, IRTools.push!(ir, :(0)))\n",
    "        factor = derivative_rule(eval(ex.args[1]), ex.args[2: length(ex.args)], i)\n",
    "        g_new = IRTools.push!(ir, :($(g_old) + $(g_assignee) * $(factor)))\n",
    "        x_to_gx[a] = g_new\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26545ac",
   "metadata": {},
   "source": [
    "The magic happens in `derivative_rule` via multiple dispatch. For each operation, we implement the derivative rule with respect to the `i`th argument. Here are the three functions we implemented in version 1, more succinctly this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fd7cc82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_rule (generic function with 3 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function derivative_rule(::typeof(sin), args, i)\n",
    "    return :(cos($(args[1])))\n",
    "end\n",
    "\n",
    "function derivative_rule(::typeof(+), args, i)\n",
    "    return 1\n",
    "end\n",
    "\n",
    "# not quite the * rule in full generality, but good enough for our two-argument purposes.\n",
    "function derivative_rule(::typeof(*), args, i)\n",
    "    if i == 1\n",
    "        return args[2]\n",
    "    elseif i == 2\n",
    "        return args[1]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10348485",
   "metadata": {},
   "source": [
    "And it still works: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a690fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.010007503399555, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff_v2.jl\")\n",
    "gradient_2(simple_math, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84b1590",
   "metadata": {},
   "source": [
    "Great! Now we have a basic reverse mode automatic differentiator, capable of handling long sequences of manipulations of primitive operations. And if the user wants to use a new function, then can define derivative_rule for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abae6d8",
   "metadata": {},
   "source": [
    "### Improvement 3: Recursive decent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096e8ea",
   "metadata": {},
   "source": [
    "Consider the following series of functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3747f643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mult (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mult(a, b)\n",
    "    a * b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e68f548e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function add(a, b)\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d2f8560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "composite (generic function with 1 method)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function composite(a, b)\n",
    "    mult(a, add(a, b))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8efb76b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function derivative_rule(::typeof(mult), args, i)\n",
    "#    return gradient_2(mult, args...)[i]\n",
    "#end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bdb0b",
   "metadata": {},
   "source": [
    "Our current implementation will fail to differentiate `composite` (with `MethodError: no method matching derivative_rule(::typeof(mult), ::Vector{Any}, ::Int64)\n",
    "`) since `add` and `mult` aren't functions it knows how to differentiate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c570234a",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] ##332",
      "    @ ./In[14]:2 [inlined]",
      "  [2] var\"##332\"(arg1::Nothing, arg2::IRTools.Inner.Variable, arg3::IRTools.Inner.Variable)",
      "    @ IRTools.Inner ~/.julia/packages/IRTools/017wp/src/eval.jl:0",
      "  [3] invokelatest(::Any, ::Any, ::Vararg{Any}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Base ./essentials.jl:729",
      "  [4] invokelatest(::Any, ::Any, ::Vararg{Any})",
      "    @ Base ./essentials.jl:726",
      "  [5] gradient_2(::Function, ::IRTools.Inner.Variable, ::Vararg{IRTools.Inner.Variable})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v2.jl:51",
      "  [6] derivative_rule(#unused#::typeof(mult), args::Vector{Any}, i::Int64)",
      "    @ Main ./In[32]:2",
      "  [7] gradient_2(::Function, ::Int64, ::Vararg{Int64})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v2.jl:29",
      "  [8] top-level scope",
      "    @ In[35]:1",
      "  [9] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "gradient_2(composite, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd55c7f",
   "metadata": {},
   "source": [
    "But we can teach it to recursively descend into `add` and `mult`, it'll discover that they're made of functions we recognize, so we should be able to differentiate the entire thing.\n",
    "\n",
    "That seems simple: we'll just introduce a simple fallback function, like this, which catches derivatives of previously unseen functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b11b0f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_rule (generic function with 4 methods)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function derivative_rule(unknown_function, args, i)\n",
    "    return gradient_2(unknown_function, args...)[i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332e4350",
   "metadata": {},
   "source": [
    "Then, instead of computing `derivative_rule` at compile time, we'll insert the `derivative_rule` symbolically and compute it at runtime. This should allow us to do things like infinite recursion without issue. Here's some SSA code generated by `gradient_3`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d222c1c3",
   "metadata": {},
   "source": [
    "```\n",
    "1: (%1, %2, %3)\n",
    "  %4 = %2 * %3\n",
    "  %5 = 1\n",
    "  %6 = 0\n",
    "  %7 = derivative_rule(*, Any[%2, %3], i)\n",
    "  %8 = %5 * %7\n",
    "  %9 = %6 + %8\n",
    "  %10 = 0\n",
    "  %11 = derivative_rule(*, Any[%2, %3], i)\n",
    "  %12 = %5 * %11\n",
    "  %13 = %10 + %12\n",
    "  %14 = Core.tuple(%4, %9, %13)\n",
    "  return %14\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b47e3d7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "derivative_rule (generic function with 4 methods)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"autodiff_v3.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240b72c5",
   "metadata": {},
   "source": [
    "Now we go to call our new SSA code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0217daaf",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n\u001b[0mClosest candidates are:\n\u001b[0m  *(::Any, ::Any, \u001b[91m::Any\u001b[39m, \u001b[91m::Any...\u001b[39m) at operators.jl:591\n\u001b[0m  *(\u001b[91m::SpecialFunctions.SimplePoly\u001b[39m, ::Any) at ~/.julia/packages/SpecialFunctions/hefUc/src/expint.jl:8\n\u001b[0m  *(\u001b[91m::ChainRulesCore.AbstractThunk\u001b[39m, ::Any) at ~/.julia/packages/ChainRulesCore/Z4Jry/src/tangent_arithmetic.jl:125\n\u001b[0m  ...",
      "",
      "Stacktrace:",
      "  [1] ##328",
      "    @ ./In[14]:2 [inlined]",
      "  [2] var\"##328\"(arg1::Nothing, arg2::IRTools.Inner.Variable, arg3::IRTools.Inner.Variable)",
      "    @ IRTools.Inner ~/.julia/packages/IRTools/017wp/src/eval.jl:0",
      "  [3] invokelatest(::Any, ::Any, ::Vararg{Any}; kwargs::Base.Pairs{Symbol, Union{}, Tuple{}, NamedTuple{(), Tuple{}}})",
      "    @ Base ./essentials.jl:729",
      "  [4] invokelatest(::Any, ::Any, ::Vararg{Any})",
      "    @ Base ./essentials.jl:726",
      "  [5] gradient_3(::Function, ::IRTools.Inner.Variable, ::Vararg{IRTools.Inner.Variable})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v3.jl:59",
      "  [6] derivative_rule(unknown_function::Function, args::Vector{Any}, i::Int64)",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v3.jl:89",
      "  [7] gradient_3(::Function, ::Int64, ::Vararg{Int64})",
      "    @ Main ~/Documents/MIT/18.S191/autodiff_v3.jl:31",
      "  [8] top-level scope",
      "    @ In[31]:1",
      "  [9] eval",
      "    @ ./boot.jl:368 [inlined]",
      " [10] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1428"
     ]
    }
   ],
   "source": [
    "gradient_3(composite, 3, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a26f6a",
   "metadata": {},
   "source": [
    "Agh! We got `MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)\n",
    "`. So we successfully recursed into `mult` and `add` and transformed the syntax, but then something fell apart when we tried to run it. What happened?\n",
    "\n",
    "Here's where we run into one of the first real issues with our divergence from Zygote's design.\n",
    "\n",
    "Our design choice was to build around a function `gradient`, which takes a function (`composite`) and two values (`3` and `5`), and returns the gradients of each argument. But we encounter a problem: internally, we need to be able to compute the gradient without inputting actual values. That means that we need to insert the `derivative_rule` function as a symbol, which is fine. But IRTools doesn't seem to support \"deep-resolving\" variables in function calls like we'd require to succesfully pass in and then resolve e.g. lists of symbolic variables.\n",
    "\n",
    "For example, I was hoping for / expecting `%7 = derivative_rule(*, Any[%2, %3], i)` to pass in the values for `%2` and `%3` at runtime, but instead we pass in the literal `::IRTools.Inner.Variable`. And then that throws `MethodError: no method matching *(::IRTools.Inner.Variable, ::IRTools.Inner.Variable)`, as expected.\n",
    "\n",
    "My current understanding is that this is a somewhat unfortunate implementation shortcoming but not fundamental limitation of `IRTools`. However, it wouldn't surprise me if there's some more fundamental reason this is impossible that I'm not seeing here.\n",
    "\n",
    "Zygote resolves this problem by building around a function called a pullback `J`, which returns an output and a gradient function. Then it can symbolically call those gradient functions in its newly constructed gradient function without issue.\n",
    "\n",
    "This is a bummer, because it means we're unlikely to be able to pull off recursion, which fundamentally requires the same trick (defer `derivative_rule` evaluation until runtime)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af6c4ad",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "This project started out with the intention of cloning `Zygote.jl`. It's no fun to clone things *exactly*, so I figured that instead of returning a value and a gradient (pullback) function, as Zygote does, I'd just compute both the value and the gradient values inline and then return both. To do so, I'd manipulate a simplified syntax (SSA-form generated via `IRTools`), as Zygote does, but inject my gradient calculations between each line instead of all at the end.\n",
    "\n",
    "This works well up to computing the derivative of \n",
    "\n",
    "At the time, I thought that would be a basically inconsequential choice, but it turned out to be more substantial than expected.\n",
    "\n",
    "When I started this project I was expecting to learn a lot about automatic differentiators. And in some sense I did--I now have a pretty good understanding of the design space and where Zygote sits in it. But really I spent most of my time (a way larger % than expected) learning how to do metaprogramming and debugging a small number of extremely dense snippets (for example, the six lines of code inside `if ex.head == :call` took *forever* to get correct.) Lots of quoting and unquoting issues in all directions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d84dd",
   "metadata": {},
   "source": [
    "### Quick note on effort allocation\n",
    "I figured I'd write a quick note on where exactly my ~3.5 psets worth of time and effort went on this project, since some parts took longer than expected and some shorter than expected.\n",
    "\n",
    "* Understanding how to implement reverse mode AD was pretty fast. I read up on it and did a few chalkboard examples, and then felt like I understood it. ~1 hour.\n",
    "* Really understanding Julia's metaprogramming (quoting, `$()`, `eval`, etc) took ~2 hours. It's been a while since I've dealt with a lispy language.\n",
    "* I spent 2-3 hours trying to inspect Zygote itself with a debugger to understand it, but this was a nightmare. One of the questions I'd like to ask someone in the Julia lab is how the pros inspect libraries in Julia. Generated functions, multiple dispatch, etc., made it extremely hard for me to track down who called whom.\n",
    "* I spent 2 hours reading and trying to understand the Zygote paper, which was actually pretty useful. In particular, when I started this project, I didn't really understand the design tradeoffs made by Zygote vs e.g. PyTorch.\n",
    "* I spent 10+ hours learning `IRTools`, and debugging various issues with it. For example, it doesn't correctly parse injected expressions like `Core.tuple(a)`, so instead we're using `IRTools.Inner.Statement(:($(GlobalRef(Core, :tuple))($(outs...),)))`, which took me forever to figure out. This was a much, much larger timesink than I expected or is probably obvious from looking at the project.\n",
    "* I spent 2-3 total hours writing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fbfc5f",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03859ee7",
   "metadata": {},
   "source": [
    "* https://arxiv.org/pdf/1810.07951.pdf. This is the original Zygote paper, and provides useful context on the tradeoffs Zygote makes in design space.\n",
    "* https://fluxml.ai/Zygote.jl/latest/internals/. This page gives an overview of how Zygote works, and includes an example of a loop being unrolled.\n",
    "* https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation. This post helped me finally understand how reverse-mode autodiff works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c290c277",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.1",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
